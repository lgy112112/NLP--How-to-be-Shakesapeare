{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory before change: /teamspace/studios/this_studio\n",
      "Current directory after change: /teamspace/studios/this_studio/NLP-Tutorial-How-to-be-Shakesapeare/Its-a-Long-Story\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current directory before change: {current_directory}\")\n",
    "\n",
    "# 要更改的目标目录\n",
    "target_directory = 'NLP-Tutorial-How-to-be-Shakesapeare/Its-a-Long-Story'\n",
    "\n",
    "# 更改当前工作目录\n",
    "os.chdir(target_directory)\n",
    "\n",
    "# 获取更改后的当前工作目录地址\n",
    "new_directory = os.getcwd()\n",
    "print(f\"Current directory after change: {new_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from tokenized_datasets...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "# 加载预处理后的数据集路径\n",
    "preprocessed_data_path = 'tokenized_datasets'\n",
    "\n",
    "# 检查是否存在已保存的预处理后的数据集\n",
    "if os.path.exists(preprocessed_data_path):\n",
    "    print(f\"Loading preprocessed data from {preprocessed_data_path}...\")\n",
    "    tokenized_datasets = load_from_disk(preprocessed_data_path)\n",
    "\n",
    "    # 选择训练集、验证集和测试集的各10000条数据\n",
    "    tokenized_datasets['train'] = tokenized_datasets['train'].select(range(5000))\n",
    "    tokenized_datasets['validation'] = tokenized_datasets['validation'].select(range(200))\n",
    "    tokenized_datasets['test'] = tokenized_datasets['test'].select(range(200))\n",
    "\n",
    "else:\n",
    "    print(\"Preprocessing data...\")\n",
    "\n",
    "    # 加载原始数据集\n",
    "    dataset = load_from_disk('filtered_dataset')\n",
    "\n",
    "    # 加载BART分词器和模型\n",
    "    model_name = 'facebook/bart-large-cnn'\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # 定义数据预处理函数，将数据转换为模型所需格式\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples['article']\n",
    "        model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "        # 设置目标 (Target) 为 highlights\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples['highlights'], max_length=128, truncation=True)\n",
    "\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    # 使用多进程进行 map 操作\n",
    "    tokenized_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=4,  # 使用的进程数量，根据你的CPU核心数调整\n",
    "        remove_columns=['article', 'highlights', 'id']\n",
    "    )\n",
    "\n",
    "    # 保存预处理后的数据集\n",
    "    tokenized_datasets.save_to_disk(preprocessed_data_path)\n",
    "    print(f\"Preprocessed data saved to {preprocessed_data_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-en-de and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import nlpaug.augmenter.word as naw\n",
    "from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments, BartForConditionalGeneration, BartTokenizer\n",
    "import sacremoses\n",
    "\n",
    "# 设置 nltk 数据下载路径\n",
    "nltk_data_path = 'nltk_data'\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# 下载所需的 nltk 数据集\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n",
    "\n",
    "# 初始化增强器\n",
    "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "random_insert_aug = naw.ContextualWordEmbsAug(action=\"insert\", model_path='bert-base-uncased')\n",
    "random_swap_aug = naw.RandomWordAug(action=\"swap\")\n",
    "random_delete_aug = naw.RandomWordAug(action=\"delete\")\n",
    "back_translation_aug = naw.BackTranslationAug(from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en')\n",
    "\n",
    "# 定义增强函数\n",
    "def augment_text(text):\n",
    "    aug_methods = [\n",
    "        synonym_aug,\n",
    "        random_insert_aug,\n",
    "        random_swap_aug,\n",
    "        random_delete_aug,\n",
    "        back_translation_aug\n",
    "    ]\n",
    "    \n",
    "    # 50%的概率进行数据增强\n",
    "    if random.random() < 0.5:\n",
    "        aug_method = random.choice(aug_methods)\n",
    "        text = aug_method.augment(text)\n",
    "    return text\n",
    "\n",
    "class DataCollatorForSeq2SeqWithAugmentation(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        for feature in features:\n",
    "            if 'article' in feature:\n",
    "                feature['input_ids'] = tokenizer(augment_text(feature['article']), max_length=1024, truncation=True)['input_ids']\n",
    "            if 'highlights' in feature:\n",
    "                feature['labels'] = tokenizer(feature['highlights'], max_length=128, truncation=True)['input_ids']\n",
    "        return super().__call__(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载BART分词器和模型\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 初始化 DataCollator\n",
    "data_collator = DataCollatorForSeq2SeqWithAugmentation(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载ROUGE评估指标\n",
    "rouge = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "# 定义计算指标的函数\n",
    "def compute_metrics(p):\n",
    "    # 取出预测的序列\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = preds.argmax(-1)\n",
    "    \n",
    "    # 解码预测的文本\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # 处理None值，避免解码错误\n",
    "    decoded_preds = [\"\".join([token if token is not None else \"\" for token in tokenizer.convert_ids_to_tokens(pred)]) for pred in preds]\n",
    "    \n",
    "    # 解码真实标签文本\n",
    "    decoded_labels = tokenizer.batch_decode(p.label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # 计算ROUGE分数\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # 返回所需的结果\n",
    "    result = {k: v.mid.fmeasure * 100 for k, v in result.items()}\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bart_results',\n",
    "    logging_dir='./bart_logs',\n",
    "    warmup_steps=500,                 # 预热步数\n",
    "    weight_decay=0.01,                # 权重衰减\n",
    "    evaluation_strategy=\"epoch\",      # 在每个 epoch 结束时进行评估\n",
    "    save_strategy=\"epoch\",            # 在每个 epoch 结束时保存模型\n",
    "    # eval_steps=10,                   # 每隔多少步进行一次评估\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# 初始化Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # 使用自定义的DataCollator\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 41:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.314600</td>\n",
       "      <td>1.758318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.904900</td>\n",
       "      <td>1.939801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.546800</td>\n",
       "      <td>2.237341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>2.608653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>2.878260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.7583175897598267, 'eval_runtime': 3.5504, 'eval_samples_per_second': 56.332, 'eval_steps_per_second': 14.083, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 训练完成后评估模型\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我为了尽快实现这个pipeline，将训练数量进行了缩减。\n",
    "\n",
    "你可以修改读取数据集那部分的代码，实现全量的训练来达到更好的效果。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
