{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory before change: /teamspace/studios/this_studio\n",
      "Directory changed to: NLP-Tutorial-How-to-be-Shakesapeare/Leave-Me-Alone\n",
      "Current directory after change: /teamspace/studios/this_studio/NLP-Tutorial-How-to-be-Shakesapeare/Leave-Me-Alone\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current directory before change: {current_directory}\")\n",
    "\n",
    "# 要更改的目标目录\n",
    "target_directory = 'NLP-Tutorial-How-to-be-Shakesapeare/Leave-Me-Alone'\n",
    "\n",
    "# 如果当前目录不是目标目录，则更改当前工作目录\n",
    "if not current_directory.endswith(target_directory):\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to: {target_directory}\")\n",
    "else:\n",
    "    print(\"Already in the target directory.\")\n",
    "\n",
    "# 获取更改后的当前工作目录地址\n",
    "new_directory = os.getcwd()\n",
    "print(f\"Current directory after change: {new_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "\n",
    "# 加载预训练的BERT分词器和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本数据转换为 BERT 所需的格式，包括掩蔽语言模型 (Masked Language Model, MLM) 和下一句预测 (Next Sentence Prediction, NSP)，是因为这些任务是 BERT 预训练的核心组成部分。\n",
    "\n",
    "### 掩蔽语言模型 (Masked Language Model, MLM)\n",
    "MLM 是 BERT 预训练过程中的关键任务。具体步骤如下：\n",
    "1. **掩蔽部分词汇**：在输入序列中随机掩蔽一些词汇，使用特殊的 `[MASK]` 标记替换它们。\n",
    "2. **预测被掩蔽的词汇**：模型尝试根据上下文预测这些被掩蔽的词汇。\n",
    "\n",
    "这种任务的好处是模型可以利用双向上下文信息来理解词汇之间的关系，而不仅仅是前向或后向单向信息。这种双向上下文的理解对于许多 NLP 任务是非常重要的。\n",
    "\n",
    "### 下一句预测 (Next Sentence Prediction, NSP)\n",
    "NSP 是 BERT 预训练过程中的另一项任务。具体步骤如下：\n",
    "1. **句子对**：对于每个训练样本，模型接收两个句子。\n",
    "2. **预测句子关系**：模型需要预测第二个句子是否是第一个句子的自然续写。\n",
    "\n",
    "这种任务的好处是模型可以学习到句子级别的关系和连贯性，从而在处理需要理解句子关系的任务（如问答和自然语言推理）时表现得更好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from preprocessed_unsupervised_data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import Dataset, load_from_disk\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# 定义清理函数\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)  # 移除HTML换行标签\n",
    "    text = re.sub(r'<.*?>', '', text)       # 移除其他HTML标签\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5A-Za-z0-9\\s]+', '', text)  # 移除特殊字符，保留中文、英文、数字和空格\n",
    "    return text\n",
    "\n",
    "# 读取处理后的CSV文件\n",
    "processed_df = pd.read_csv('processed_unsupervised_data.csv')\n",
    "processed_df['text'] = processed_df['text'].apply(clean_text)\n",
    "\n",
    "# 转换为Dataset对象\n",
    "unsupervised_dataset = Dataset.from_pandas(processed_df)\n",
    "\n",
    "# 定义数据预处理函数\n",
    "def preprocess_data(examples):\n",
    "    encoding = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "    input_ids = torch.tensor(encoding['input_ids'])\n",
    "    \n",
    "    # 创建掩蔽语言模型任务的标签\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # 80%的时间替换为[Mask]\n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < 0.15) * (input_ids != tokenizer.cls_token_id) * (input_ids != tokenizer.sep_token_id) * (input_ids != tokenizer.pad_token_id)\n",
    "    selection = []\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        input_ids[i, selection[i]] = tokenizer.mask_token_id\n",
    "\n",
    "    # 创建下一句预测的标签\n",
    "    next_sentence_label = torch.zeros(input_ids.shape[0], dtype=torch.long)\n",
    "    \n",
    "    encoding['labels'] = labels.tolist()  # 将 tensor 转回列表\n",
    "    encoding['input_ids'] = input_ids.tolist()  # 将 tensor 转回列表\n",
    "    encoding['next_sentence_label'] = next_sentence_label.tolist()\n",
    "    return encoding\n",
    "\n",
    "# 检查是否存在预处理后的数据集\n",
    "preprocessed_data_path = 'preprocessed_unsupervised_data'\n",
    "\n",
    "if os.path.exists(preprocessed_data_path):\n",
    "    print(f\"Loading preprocessed data from {preprocessed_data_path}...\")\n",
    "    unsupervised_dataset = load_from_disk(preprocessed_data_path)\n",
    "else:\n",
    "    print(\"Preprocessing data...\")\n",
    "    # 预处理数据集\n",
    "    unsupervised_dataset = unsupervised_dataset.map(preprocess_data, batched=True)\n",
    "    unsupervised_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'next_sentence_label'])\n",
    "    \n",
    "    # 保存预处理后的数据集\n",
    "    unsupervised_dataset.save_to_disk(preprocessed_data_path)\n",
    "    print(f\"Preprocessed data saved to {preprocessed_data_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 从磁盘加载预处理后的数据集\n",
    "preprocessed_data_path = 'preprocessed_unsupervised_data'\n",
    "unsupervised_dataset = load_from_disk(preprocessed_data_path)\n",
    "\n",
    "# 设置数据格式\n",
    "unsupervised_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'next_sentence_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForPreTraining, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# 定义计算指标的函数\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions[0], axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# 自定义 Trainer 以重写 compute_loss 方法\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        next_sentence_label = inputs.pop(\"next_sentence_label\")\n",
    "        outputs = model(**inputs)\n",
    "        prediction_logits = outputs.prediction_logits\n",
    "        seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        \n",
    "        # 计算掩蔽语言模型的损失\n",
    "        mlm_loss = torch.nn.functional.cross_entropy(prediction_logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "        \n",
    "        # 计算下一句预测的损失\n",
    "        nsp_loss = torch.nn.functional.cross_entropy(seq_relationship_logits.view(-1, 2), next_sentence_label.view(-1))\n",
    "        \n",
    "        loss = mlm_loss + nsp_loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 加载预训练的 BERT 模型\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./pretrain_results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    logging_dir='./pretrain_logs',\n",
    "    logging_steps=500,                 # 记录日志的步数\n",
    "    evaluation_strategy=\"no\",          # 暂时禁用评估\n",
    "    save_strategy=\"epoch\",             # 在每个 epoch 结束时保存模型\n",
    "    fp16=True                          # 启用自动混合精度\n",
    ")\n",
    "\n",
    "# 创建一个伪的评估数据集\n",
    "dummy_data = {'input_ids': [[0]], 'attention_mask': [[0]], 'labels': [[0]], 'next_sentence_label': [0]}\n",
    "dummy_eval_dataset = Dataset.from_dict(dummy_data)\n",
    "\n",
    "# 初始化 CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=unsupervised_dataset,\n",
    "    eval_dataset=dummy_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Original text: this is just a little the play the script are excellent i can compare this movie anything else maybe except the movie leonly played by jean reno and natalieman but what can say about this one this is best movie anne parillaud played in please frankie shes speaking english there to see i mean the story young punk girl nikita into the depraved world the secret government forces has exceptionally over used by americans never the of no return and the la femme nikita tv they cannot believe me videos buy this one do not it buy it btwware of the subtitles of the company translate us release a disgrace if you can understand french get a dubbed youll later\n",
      "Predicted text: this is just a nice little movie the play the script are excellent i cant compare this movie to anything else maybe except the movie leon brilliantly played by jean reno and natalie portman but what can i say about this one this is the best movie anne parillaud has ever played in and please frankie avalon think shes speaking english there to see what i mean the story of young punk girl nikita thrown into the depraved world of the secret government forces has been exceptionally over used by americans never mind the point of no return and unlike the la femme nikita tv series they cannot get it it believe me buy the videos buy this one do not rent it buy it btw beware of the subtitles of the production company to translate the us release what a disgrace if you cant understand french get a dubbed version and youll laugh later\n",
      "Sequence relationship prediction: 0\n",
      "\n",
      "Sample 2:\n",
      "Original text: when i say this is my favourite of time that comment is not to taken lightly i probably watch far films than is healthy for me and have loved quite a few of them i saw la femme nikita nearly years ago it still to my absolute favourite why this is more than an incredibly stylish and sexy thriller luc besson great flair for impeccable direction fashion and appropriate usage of makes this a very watchable film but anne parillauds perfect rendering of complex character who transforms from a heartless killer into compassionate young woman that makes film beautiful i cant keep eyes off of her when she is on screen i seen of bessons including subway the professional and the irritating fifth element and nikita without a doubt far superior any of these although this film has it is ultimately extremely hopeful it is of who is cruel and merciless who ultimately comes to realize her own humanity and her own personal power that to me is extremely inspiring if there hope for nikita is hope for all of\n",
      "Predicted text: when i say this is my favourite film of all time that comment is not to be taken lightly i probably watch far more more films than is healthy for me and have loved quite a few of them i first saw la femme nikita nearly 20 years ago and it still manages to be my absolute favourite why this is more than an incredibly stylish and sexy thriller luc bessons great flair for impeccable direction fashion and appropriate usage of color makes this a very watchable film but it is anne parillauds perfect rendering of a complex character who transforms from a heartless killer into a compassionate caring young woman that makes this film beautiful i cant keep my eyes off of her when she is on screen i have seen many of luc bessons films including subway the professional and the irritating fifth element and nikita is without a doubt far superior to any of these although this film has no flaws it is ultimately extremely hopeful it is the portrayal of a woman who is cruel and merciless who ultimately comes to realize her own humanity and her own personal power that to me is extremely inspiring if there is hope for nikita there is hope for all of us\n",
      "Sequence relationship prediction: 0\n",
      "\n",
      "Sample 3:\n",
      "Original text: saw this movie because a huge the tv series of the same name starring roy dupuis and wilson the was good i saw how the tv show is based the a few episodes of tv series came directly from the movie and their was keep things short any fan of the movie has to watch the series and any fan of the series must see the original nikita\n",
      "Predicted text: i saw this movie because i was a huge fan of the tv series of the same name starring roy dupuis and julie wilson the movie was very good and i saw how the tv show is based on the movie a few episodes of the tv series came directly from the movie and their story was great to keep things short any fan of the movie has to watch the series and any fan of the series must see the original nikita\n",
      "Sequence relationship prediction: 0\n",
      "\n",
      "Sample 4:\n",
      "Original text: being that the foreign films i star a japanese person in a rubber suit who crush little tiny buildings and i hopes this movie i that this was a movie that wouldnt put me to sleep wrong starts off with a bang okay now shes in training alright shes an assassin im with you ohs having this moral dilemma she cant decide if she her or her controller zzzzz oh well back to gamera\n",
      "Predicted text: being that the only foreign films i know see star a japanese person in a rubber suit who crushes little tiny buildings and people i had high hopes for this movie i thought that this was a movie that wouldnt put me to sleep wrong starts off with a bang okay now shes in training alright shes an assassin im coming with you oh well shes having this moral dilemma where she cant decide if she loves her gun or her controller zzzzz oh well back to gamera\n",
      "Sequence relationship prediction: 0\n",
      "\n",
      "Sample 5:\n",
      "Original text: after seeing point return a great movie and being told that the was better i was certainly thrilled to see that of indie film channels was running la nikita then i saw the movie ouch this was major letdown nikita herself reminds me jar jar binks more than any other character ive seen recently she comes across entirely comic relief the movie simply has nothing to recommend it the core concept of an inhuman character paradoxically learning to be human while training as an assassin and that failed misbly in nikita due to the poor writing the title role\n",
      "Predicted text: after seeing point of no return a great movie and being told that the acting was better i was certainly thrilled to see that one of the indie film channels was running la femme nikita then i saw the movie ouch this was a major letdown nikita herself reminds me of jar jar binks more than any other character ive seen recently she comes across entirely as comic relief the movie simply has nothing to recommend it besides the core concept of an almost inhuman character paradoxically learning to be human while training as an assassin and that concept failed miserably in nikita due to the poor writing in the title role\n",
      "Sequence relationship prediction: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# 从磁盘加载预处理后的数据集\n",
    "preprocessed_data_path = 'preprocessed_unsupervised_data'\n",
    "unsupervised_dataset = load_from_disk(preprocessed_data_path)\n",
    "\n",
    "# 设置数据格式\n",
    "unsupervised_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'next_sentence_label'])\n",
    "\n",
    "# 加载训练好的模型和分词器\n",
    "model_path = 'pretrain_results/checkpoint-8682'\n",
    "model = BertForPreTraining.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 选择一些样本进行预测\n",
    "sample_indices = [0, 1, 2, 3, 4]  # 可以根据需要调整索引\n",
    "samples = unsupervised_dataset.select(sample_indices)\n",
    "\n",
    "# 将数据加载到模型中进行预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(samples):\n",
    "        input_ids = sample['input_ids'].unsqueeze(0)  # 增加batch维度\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0)\n",
    "        labels = sample['labels'].unsqueeze(0)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        prediction_logits = outputs.prediction_logits\n",
    "        seq_relationship_logits = outputs.seq_relationship_logits\n",
    "\n",
    "        # 解码预测结果\n",
    "        predicted_tokens = torch.argmax(prediction_logits, dim=-1)\n",
    "        predicted_text = tokenizer.decode(predicted_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\"Original text: {tokenizer.decode(input_ids[0], skip_special_tokens=True)}\")\n",
    "        print(f\"Predicted text: {predicted_text}\")\n",
    "        print(f\"Sequence relationship prediction: {torch.argmax(seq_relationship_logits, dim=-1).item()}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解释\n",
    "\n",
    "在这段代码中，我们使用了预训练后的BERT模型对一些样本进行了预测，并打印了原始文本、预测文本以及序列关系预测的结果。以下是输出的详细解释：\n",
    "\n",
    "#### 输出示例\n",
    "\n",
    "**Sample 1**:\n",
    "- **Original text (原始文本)**:\n",
    "  ```text\n",
    "  this is just a little the play the script are excellent i can compare this movie anything else maybe except the movie leonly played by jean reno and natalieman but what can say about this one this is best movie anne parillaud played in please frankie shes speaking english there to see i mean the story young punk girl nikita into the depraved world the secret government forces has exceptionally over used by americans never the of no return and the la femme nikita tv they cannot believe me videos buy this one do not it buy it btwware of the subtitles of the company translate us release a disgrace if you can understand french get a dubbed youll later\n",
    "  ```\n",
    "  这是从数据集中提取的原始文本，包含了一些拼写错误和语法错误，显得有些混乱。\n",
    "\n",
    "- **Predicted text (预测文本)**:\n",
    "  ```text\n",
    "  this is just a nice little movie the play the script are excellent i cant compare this movie to anything else maybe except the movie leon brilliantly played by jean reno and natalie portman but what can i say about this one this is the best movie anne parillaud has ever played in and please frankie avalon think shes speaking english there to see what i mean the story of young punk girl nikita thrown into the depraved world of the secret government forces has been exceptionally over used by americans never mind the point of no return and unlike the la femme nikita tv series they cannot get it it believe me buy the videos buy this one do not rent it buy it btw beware of the subtitles of the production company to translate the us release what a disgrace if you cant understand french get a dubbed version and youll laugh later\n",
    "  ```\n",
    "  这是模型根据原始文本预测出的文本。可以看到，模型在一定程度上改正了一些拼写错误和语法错误，使得文本更通顺。例如，将 \"leonly\" 更正为 \"leon\"，将 \"natalieman\" 更正为 \"natalie portman\"。\n",
    "\n",
    "- **Sequence relationship prediction (序列关系预测)**: `0`\n",
    "  这是模型对下一句预测任务的结果。BERT模型通常会进行两个任务：\n",
    "  - 掩蔽语言模型 (Masked Language Model, MLM)：预测被掩蔽的词。\n",
    "  - 下一句预测 (Next Sentence Prediction, NSP)：判断两段文本是否相邻。\n",
    "  \n",
    "  序列关系预测的结果是 `0`，这通常表示模型预测这两段文本不是相邻的句子。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些文件是使用 `transformers` 库训练模型后生成的检查点文件。这些文件保存了模型的各种状态和参数，以便于在训练过程中断点续训或进行评估。\n",
    "\n",
    "1. **`config.json`**:\n",
    "   - 这个文件包含了模型的配置参数。它定义了模型的架构和超参数，例如层数、隐藏层大小、注意力头数等。\n",
    "   - 在加载模型时，这个文件用于重新构建模型架构。\n",
    "\n",
    "2. **`model.safetensors`**:\n",
    "   - 这个文件包含了训练好的模型权重。它是模型实际的参数，用于进行推理和进一步的训练。\n",
    "   - 这个文件的格式是 `.safetensors`，它是一个高效的二进制格式，用于保存大规模张量数据。\n",
    "\n",
    "3. **`optimizer.pt`**:\n",
    "   - 这个文件保存了优化器的状态。优化器的状态包括动量和学习率调度器等信息。\n",
    "   - 在恢复训练时，这个文件用于恢复优化器的状态，使得训练可以从中断的地方继续。\n",
    "\n",
    "4. **`rng_state.pth`**:\n",
    "   - 这个文件保存了随机数生成器的状态，包括 PyTorch 和 NumPy 的随机数生成器状态。\n",
    "   - 这样可以确保训练过程中断点续训时，随机数序列的一致性，从而保证结果的可重复性。\n",
    "\n",
    "5. **`scheduler.pt`**:\n",
    "   - 这个文件保存了学习率调度器的状态。学习率调度器用于动态调整训练过程中的学习率。\n",
    "   - 在恢复训练时，这个文件用于恢复学习率调度器的状态。\n",
    "\n",
    "6. **`trainer_state.json`**:\n",
    "   - 这个文件保存了训练器（Trainer）的状态，包括当前的步数、损失值等信息。\n",
    "   - 在恢复训练时，这个文件用于恢复训练器的状态，使得训练可以从中断的地方继续。\n",
    "\n",
    "7. **`training_args.bin`**:\n",
    "   - 这个文件保存了训练参数。这些参数定义了训练过程中的各种配置，例如批处理大小、学习率、训练轮数等。\n",
    "   - 在恢复训练时，这个文件用于恢复训练参数，使得训练可以从中断的地方继续。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size of the files in the directory 'pretrain_results/checkpoint-8682' is 1.23 GB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "def format_size(size):\n",
    "    # 将字节大小格式化为更易读的格式\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size < 1024:\n",
    "            return f\"{size:.2f} {unit}\"\n",
    "        size /= 1024\n",
    "\n",
    "# 指定目录\n",
    "directory = 'pretrain_results/checkpoint-8682'\n",
    "\n",
    "# 计算目录总大小\n",
    "total_size = get_directory_size(directory)\n",
    "\n",
    "# 格式化大小并输出\n",
    "formatted_size = format_size(total_size)\n",
    "print(f\"The total size of the files in the directory '{directory}' is {formatted_size}.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
